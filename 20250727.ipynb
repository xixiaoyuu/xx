{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd36298d-c12c-4fef-a096-52abecaef8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Sequence vocabulary size: 21\n",
      "Loaded TSV with columns: Entry, Protein names, Sequence\n",
      "Extracted 20420 raw sequence-description pairs\n",
      "Filtered to 5260 valid pairs (length 20~256)\n",
      "Applying data augmentation...\n",
      "Augmented dataset size: 18936\n",
      "\n",
      "Text vocabulary size: 5000\n",
      "Top 10 words: ['(', ')', 'protein', 'variant', '1', '2', '.', '3', 'subunit', 'ec']\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/50\n",
      "Train Loss: 0.5674 | Val Loss: 0.3721\n",
      "Saved best model (val loss improved)\n",
      "Epoch 2/50\n",
      "Train Loss: 0.3799 | Val Loss: 0.3217\n",
      "Saved best model (val loss improved)\n",
      "Epoch 3/50\n",
      "Train Loss: 0.3206 | Val Loss: 0.2737\n",
      "Saved best model (val loss improved)\n",
      "Epoch 4/50\n",
      "Train Loss: 0.2798 | Val Loss: 0.2486\n",
      "Saved best model (val loss improved)\n",
      "Epoch 5/50\n",
      "Train Loss: 0.2481 | Val Loss: 0.2156\n",
      "Saved best model (val loss improved)\n",
      "Epoch 6/50\n",
      "Train Loss: 0.2150 | Val Loss: 0.1833\n",
      "Saved best model (val loss improved)\n",
      "Epoch 7/50\n",
      "Train Loss: 0.1933 | Val Loss: 0.1584\n",
      "Saved best model (val loss improved)\n",
      "Epoch 8/50\n",
      "Train Loss: 0.1714 | Val Loss: 0.1514\n",
      "Saved best model (val loss improved)\n",
      "Epoch 9/50\n",
      "Train Loss: 0.1524 | Val Loss: 0.1464\n",
      "Saved best model (val loss improved)\n",
      "Epoch 10/50\n",
      "Train Loss: 0.1372 | Val Loss: 0.1178\n",
      "Saved best model (val loss improved)\n",
      "Epoch 11/50\n",
      "Train Loss: 0.1245 | Val Loss: 0.1167\n",
      "Saved best model (val loss improved)\n",
      "Epoch 12/50\n",
      "Train Loss: 0.1103 | Val Loss: 0.1005\n",
      "Saved best model (val loss improved)\n",
      "Epoch 13/50\n",
      "Train Loss: 0.0987 | Val Loss: 0.0919\n",
      "Saved best model (val loss improved)\n",
      "Epoch 14/50\n",
      "Train Loss: 0.0901 | Val Loss: 0.0791\n",
      "Saved best model (val loss improved)\n",
      "Epoch 15/50\n",
      "Train Loss: 0.0811 | Val Loss: 0.0748\n",
      "Saved best model (val loss improved)\n",
      "Epoch 16/50\n",
      "Train Loss: 0.0732 | Val Loss: 0.0706\n",
      "Saved best model (val loss improved)\n",
      "Epoch 17/50\n",
      "Train Loss: 0.0654 | Val Loss: 0.0611\n",
      "Saved best model (val loss improved)\n",
      "Epoch 18/50\n",
      "Train Loss: 0.0577 | Val Loss: 0.0524\n",
      "Saved best model (val loss improved)\n",
      "Epoch 19/50\n",
      "Train Loss: 0.0518 | Val Loss: 0.0528\n",
      "Epoch 20/50\n",
      "Train Loss: 0.0466 | Val Loss: 0.0473\n",
      "Saved best model (val loss improved)\n",
      "Epoch 21/50\n",
      "Train Loss: 0.0409 | Val Loss: 0.0407\n",
      "Saved best model (val loss improved)\n",
      "Epoch 22/50\n",
      "Train Loss: 0.0365 | Val Loss: 0.0360\n",
      "Saved best model (val loss improved)\n",
      "Epoch 23/50\n",
      "Train Loss: 0.0319 | Val Loss: 0.0323\n",
      "Saved best model (val loss improved)\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0279 | Val Loss: 0.0281\n",
      "Saved best model (val loss improved)\n",
      "Epoch 25/50\n",
      "Train Loss: 0.0243 | Val Loss: 0.0248\n",
      "Saved best model (val loss improved)\n",
      "Epoch 26/50\n",
      "Train Loss: 0.0210 | Val Loss: 0.0216\n",
      "Saved best model (val loss improved)\n",
      "Epoch 27/50\n",
      "Train Loss: 0.0182 | Val Loss: 0.0183\n",
      "Saved best model (val loss improved)\n",
      "Epoch 28/50\n",
      "Train Loss: 0.0156 | Val Loss: 0.0157\n",
      "Saved best model (val loss improved)\n",
      "Epoch 29/50\n",
      "Train Loss: 0.0134 | Val Loss: 0.0138\n",
      "Saved best model (val loss improved)\n",
      "Epoch 30/50\n",
      "Train Loss: 0.0113 | Val Loss: 0.0112\n",
      "Saved best model (val loss improved)\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0096 | Val Loss: 0.0092\n",
      "Saved best model (val loss improved)\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0079 | Val Loss: 0.0077\n",
      "Saved best model (val loss improved)\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0066 | Val Loss: 0.0063\n",
      "Saved best model (val loss improved)\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0054 | Val Loss: 0.0051\n",
      "Saved best model (val loss improved)\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0044 | Val Loss: 0.0042\n",
      "Saved best model (val loss improved)\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0036 | Val Loss: 0.0034\n",
      "Saved best model (val loss improved)\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0028 | Val Loss: 0.0027\n",
      "Saved best model (val loss improved)\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0023 | Val Loss: 0.0022\n",
      "Saved best model (val loss improved)\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0018 | Val Loss: 0.0017\n",
      "Saved best model (val loss improved)\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Saved best model (val loss improved)\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0011 | Val Loss: 0.0010\n",
      "Saved best model (val loss improved)\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0008 | Val Loss: 0.0008\n",
      "Saved best model (val loss improved)\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0006 | Val Loss: 0.0006\n",
      "Saved best model (val loss improved)\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0005 | Val Loss: 0.0004\n",
      "Saved best model (val loss improved)\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0004 | Val Loss: 0.0003\n",
      "Saved best model (val loss improved)\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0003 | Val Loss: 0.0002\n",
      "Saved best model (val loss improved)\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0002 | Val Loss: 0.0002\n",
      "Saved best model (val loss improved)\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0002 | Val Loss: 0.0001\n",
      "Saved best model (val loss improved)\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0001 | Val Loss: 0.0001\n",
      "Saved best model (val loss improved)\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0001 | Val Loss: 0.0001\n",
      "Saved best model (val loss improved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\32816\\AppData\\Local\\Temp\\ipykernel_39888\\1122325614.py:447: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_protein_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to 'protein_diffusion_model_final.pth'\n",
      "\n",
      "Generating sample sequences...\n",
      "\n",
      "Description: DNA binding protein involved in transcription\n",
      "Generated sequence: QWMHNPMWWAMYCYAIYMWAFRMHWWYHWMDCRFEILVYYNWMHAERHEIWREFNEFRFKPAMFTHANEMEVMERAPYDMMEGYCEGYFRCLQMRWQQLG...\n",
      "Sequence length: 237\n",
      "\n",
      "Description: Enzyme with catalytic activity for hydrolysis\n",
      "Generated sequence: ASRSNYIFGEKRSQETDDYNHADHGEEHAMLHYYKSFYKTDNWGMEEWGPPHEMAYDAHLANCWHWWTYFGEIIYEFYAYYLDLQFPQMFWTQNAHMIHR...\n",
      "Sequence length: 244\n",
      "\n",
      "Description: Membrane transport protein for ions\n",
      "Generated sequence: KCGPRLEHPSHKCQWWYEEINDCNPCIFHEERPYHMKMMQENYRRFMYVESDSWHQCLFGMGDGNYYFGYYCDSAGVWGEGMRVTRYKFWWRFYGHFRSK...\n",
      "Sequence length: 244\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# 超参数配置\n",
    "class Config:\n",
    "    # 序列相关参数\n",
    "    seq_vocab_size = 21  # 20种氨基酸 + 1个填充位置\n",
    "    max_seq_len = 256\n",
    "    seq_embed_dim = 128\n",
    "    \n",
    "    # 文本相关参数（单词级）\n",
    "    text_vocab_size = 5000  # 单词词汇表大小\n",
    "    max_text_len = 128     # 单词序列长度（更短）\n",
    "    text_embed_dim = 128\n",
    "    \n",
    "    # 其他共享参数\n",
    "    time_embed_dim = 64\n",
    "    hidden_dim = 256\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    dropout = 0.3\n",
    "    beta_start = 1e-4\n",
    "    beta_end = 0.02\n",
    "    T = 1000\n",
    "    batch_size = 32\n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "    val_split = 0.1\n",
    "    # 数据增强参数\n",
    "    augmentation_factor = 3  # 每条序列增强的次数\n",
    "    mutation_rate = 0.01     # 每个氨基酸的变异概率\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# 文本分词器（单词级）\n",
    "class WordTokenizer:\n",
    "    def __init__(self, word_counts=None):\n",
    "        # 特殊标记\n",
    "        self.special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        \n",
    "        # 构建词汇表\n",
    "        self.vocab = self.special_tokens.copy()\n",
    "        \n",
    "        # 添加高频词\n",
    "        if word_counts:\n",
    "            sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            for word, _ in sorted_words:\n",
    "                if len(self.vocab) < config.text_vocab_size:\n",
    "                    self.vocab.append(word)\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        # 创建映射\n",
    "        for i, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = i\n",
    "            self.idx2word[i] = word\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"将文本分割为单词/标点序列\"\"\"\n",
    "        # 匹配单词（含连字符）和标点\n",
    "        return re.findall(r\"\\b[\\w'-]+\\b|[^\\w\\s]\", text.lower())\n",
    "    \n",
    "    def encode(self, text, max_len=None):\n",
    "        max_len = max_len or config.max_text_len\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        # 添加起始和结束标记\n",
    "        tokens = [\"<sos>\"] + tokens[:max_len-2] + [\"<eos>\"]\n",
    "        \n",
    "        # 转换为索引\n",
    "        encoded = []\n",
    "        for token in tokens:\n",
    "            if token in self.word2idx:\n",
    "                encoded.append(self.word2idx[token])\n",
    "            else:\n",
    "                encoded.append(self.word2idx[\"<unk>\"])\n",
    "        \n",
    "        # 填充或截断\n",
    "        if len(encoded) < max_len:\n",
    "            encoded += [self.word2idx[\"<pad>\"]] * (max_len - len(encoded))\n",
    "        else:\n",
    "            encoded = encoded[:max_len-1] + [self.word2idx[\"<eos>\"]]  # 确保结束标记\n",
    "        \n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"将索引序列解码为文本\"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            word = self.idx2word.get(idx, \"<unk>\")\n",
    "            if word in [\"<pad>\", \"<sos>\", \"<eos>\"]:\n",
    "                continue\n",
    "            words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "# 蛋白质序列分词器\n",
    "class SequenceTokenizer:\n",
    "    def __init__(self):\n",
    "        self.aa_list = \"ACDEFGHIKLMNPQRSTVWY\"  # 20种标准氨基酸\n",
    "        self.pad_token = \"<pad>\"\n",
    "        \n",
    "        self.aa2idx = {self.pad_token: 0}\n",
    "        for aa in self.aa_list:\n",
    "            self.aa2idx[aa] = len(self.aa2idx)\n",
    "        \n",
    "        self.idx2aa = {v: k for k, v in self.aa2idx.items()}\n",
    "        self.vocab_size = len(self.aa2idx)\n",
    "    \n",
    "    def encode(self, sequence, max_len=None):\n",
    "        max_len = max_len or config.max_seq_len\n",
    "        encoded = []\n",
    "        for aa in sequence[:max_len]:\n",
    "            encoded.append(self.aa2idx.get(aa, self.aa2idx[self.pad_token]))\n",
    "        \n",
    "        if len(encoded) < max_len:\n",
    "            encoded += [self.aa2idx[self.pad_token]] * (max_len - len(encoded))\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx2aa.get(idx, 'X') for idx in indices if idx != self.aa2idx[self.pad_token]])\n",
    "\n",
    "# 时间步嵌入\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        # 计算频率\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        # 时间步与频率相乘\n",
    "        embeddings = t[:, None] * embeddings[None, :]\n",
    "        # 拼接正弦和余弦\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "# 文本编码器\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 单词嵌入层\n",
    "        self.embedding = nn.Embedding(config.text_vocab_size, config.text_embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Transformer编码器\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.text_embed_dim,\n",
    "            nhead=config.num_heads,\n",
    "            dim_feedforward=config.hidden_dim,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, config.num_layers)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        encoded = self.encoder(embedded)\n",
    "        return encoded\n",
    "\n",
    "# 蛋白质扩散模型\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, config, text_tokenizer, seq_tokenizer):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.seq_tokenizer = seq_tokenizer\n",
    "        self.pad_idx = seq_tokenizer.aa2idx[\"<pad>\"]  # 序列填充索引\n",
    "        \n",
    "        # 文本条件编码器\n",
    "        self.text_encoder = TextEncoder(config)\n",
    "        \n",
    "        # 时间步嵌入\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(config.time_embed_dim),\n",
    "            nn.Linear(config.time_embed_dim, config.time_embed_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # 序列嵌入层（使用蛋白质词汇表）\n",
    "        self.seq_embed = nn.Embedding(\n",
    "            seq_tokenizer.vocab_size, \n",
    "            config.seq_embed_dim,\n",
    "            padding_idx=self.pad_idx\n",
    "        )\n",
    "        \n",
    "        # 条件融合：融合文本编码和时间嵌入\n",
    "        self.condition_fuse = nn.Sequential(\n",
    "            nn.Linear(config.text_embed_dim + config.time_embed_dim, config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_dim, config.seq_embed_dim)  # 映射到序列嵌入维度\n",
    "        )\n",
    "        \n",
    "        # Transformer解码器（用于去噪）\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=config.seq_embed_dim,\n",
    "            nhead=config.num_heads,\n",
    "            dim_feedforward=config.hidden_dim,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, config.num_layers)\n",
    "        \n",
    "        # 输出层：将嵌入映射到氨基酸词汇表\n",
    "        self.output_layer = nn.Linear(config.seq_embed_dim, seq_tokenizer.vocab_size)\n",
    "        \n",
    "        # 初始化扩散参数（beta/alpha系列）\n",
    "        self.register_buffer('betas', torch.linspace(config.beta_start, config.beta_end, config.T))\n",
    "        self.register_buffer('alphas', 1. - self.betas)\n",
    "        self.register_buffer('alphas_bar', torch.cumprod(self.alphas, dim=0))  # 累积乘积\n",
    "        self.register_buffer('sqrt_alphas_bar', torch.sqrt(self.alphas_bar))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_bar', torch.sqrt(1. - self.alphas_bar))\n",
    "    \n",
    "    def forward_emb(self, xt_emb, t, text_condition):\n",
    "        \"\"\"处理加噪后的嵌入，预测原始嵌入（核心前向逻辑）\"\"\"\n",
    "        # 1. 时间步嵌入\n",
    "        t_emb = self.time_embed(t)  # (batch, time_embed_dim)\n",
    "        \n",
    "        # 2. 文本条件编码\n",
    "        text_encoded = self.text_encoder(text_condition)  # (batch, text_len, text_embed_dim)\n",
    "        \n",
    "        # 3. 融合文本和时间条件\n",
    "        # 文本编码取平均作为全局特征\n",
    "        text_avg = text_encoded.mean(dim=1)  # (batch, text_embed_dim)\n",
    "        # 拼接文本和时间嵌入\n",
    "        cond = torch.cat([text_avg, t_emb], dim=-1)  # (batch, text_embed_dim + time_embed_dim)\n",
    "        cond = self.condition_fuse(cond)  # (batch, seq_embed_dim)\n",
    "        # 扩展到序列长度（与xt_emb对齐）\n",
    "        cond = cond.unsqueeze(1).repeat(1, xt_emb.size(1), 1)  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "        # 4. 融合加噪嵌入与条件\n",
    "        x_cond = xt_emb + cond  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "        # 5. Transformer解码（用文本编码作为memory增强条件）\n",
    "        output_emb = self.decoder(tgt=x_cond, memory=text_encoded)  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "        return output_emb\n",
    "\n",
    "    def p_loss(self, x0, text_condition):\n",
    "        \"\"\"计算扩散损失（基于嵌入空间的MSE）\"\"\"\n",
    "        batch_size = x0.size(0)\n",
    "        device = x0.device\n",
    "        \n",
    "        # 1. 随机采样时间步\n",
    "        t = torch.randint(0, self.config.T, (batch_size,), device=device)\n",
    "        \n",
    "        # 2. 原始序列嵌入\n",
    "        x0_emb = self.seq_embed(x0)  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "        # 3. 前向扩散：在嵌入空间添加噪声\n",
    "        noise = torch.randn_like(x0_emb)  # 高斯噪声\n",
    "        # 提取当前时间步的系数\n",
    "        sqrt_alpha_bar = self.sqrt_alphas_bar[t][:, None, None]  # (batch, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar = self.sqrt_one_minus_alphas_bar[t][:, None, None]  # (batch, 1, 1)\n",
    "        # 计算加噪嵌入xt_emb\n",
    "        xt_emb = sqrt_alpha_bar * x0_emb + sqrt_one_minus_alpha_bar * noise  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "        # 4. 模型预测原始嵌入\n",
    "        pred_x0_emb = self.forward_emb(xt_emb, t, text_condition)  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "        # 5. 计算MSE损失（忽略填充位置）\n",
    "        mask = (x0 != self.pad_idx).float().unsqueeze(-1)  # (batch, seq_len, 1)：填充位置为0\n",
    "        loss = F.mse_loss(pred_x0_emb * mask, x0_emb * mask, reduction='mean')\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, text_condition):\n",
    "        \"\"\"反向扩散采样：从噪声嵌入生成蛋白质序列\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = text_condition.size(0)\n",
    "        seq_len = self.config.max_seq_len\n",
    "        \n",
    "        # 1. 初始噪声嵌入（从标准正态分布采样）\n",
    "        xt_emb = torch.randn((batch_size, seq_len, self.config.seq_embed_dim), device=device)\n",
    "        \n",
    "        # 2. 逐步去噪\n",
    "        for t_step in reversed(range(self.config.T)):\n",
    "            t = torch.full((batch_size,), t_step, device=device, dtype=torch.long)  # 当前时间步\n",
    "            \n",
    "            # 预测原始嵌入\n",
    "            pred_x0_emb = self.forward_emb(xt_emb, t, text_condition)  # (batch, seq_len, seq_embed_dim)\n",
    "            \n",
    "            # 计算反向扩散系数\n",
    "            alpha = self.alphas[t_step]\n",
    "            alpha_bar = self.alphas_bar[t_step]\n",
    "            beta = self.betas[t_step]\n",
    "            \n",
    "            # 采样噪声（最后一步用0噪声）\n",
    "            if t_step > 0:\n",
    "                noise = torch.randn_like(xt_emb)\n",
    "            else:\n",
    "                noise = torch.zeros_like(xt_emb)\n",
    "            \n",
    "            # 反向更新公式：xt-1 = (1/sqrt(alpha)) * (xt - (1-alpha)/sqrt(1-alpha_bar) * pred_x0) + sqrt(beta) * noise\n",
    "            xt_emb = (1 / torch.sqrt(alpha)) * (\n",
    "                xt_emb - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * pred_x0_emb\n",
    "            ) + torch.sqrt(beta) * noise\n",
    "        \n",
    "        # 3. 将最终嵌入映射到氨基酸索引\n",
    "        logits = self.output_layer(xt_emb)  # (batch, seq_len, vocab_size)\n",
    "        return torch.argmax(logits, dim=-1)  # 取概率最大的索引\n",
    "\n",
    "# 数据增强函数\n",
    "def augment_protein_data(sequences, descriptions, config):\n",
    "    \"\"\"通过轻微变异增强蛋白质序列数据\"\"\"\n",
    "    augmented_seqs = []\n",
    "    augmented_descs = []\n",
    "    \n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    \n",
    "    for seq, desc in zip(sequences, descriptions):\n",
    "        # 添加原始序列\n",
    "        augmented_seqs.append(seq)\n",
    "        augmented_descs.append(desc)\n",
    "        \n",
    "        # 创建变异序列\n",
    "        for i in range(config.augmentation_factor):\n",
    "            seq_list = list(seq)\n",
    "            \n",
    "            # 随机替换氨基酸\n",
    "            for j in range(len(seq_list)):\n",
    "                if random.random() < config.mutation_rate:\n",
    "                    current_aa = seq_list[j]\n",
    "                    possible_replacements = [aa for aa in amino_acids if aa != current_aa]\n",
    "                    if possible_replacements:\n",
    "                        seq_list[j] = random.choice(possible_replacements)\n",
    "            \n",
    "            augmented_seqs.append(''.join(seq_list))\n",
    "            # 在描述中添加变体标记\n",
    "            augmented_descs.append(f\"{desc} (variant {i+1})\")\n",
    "    \n",
    "    return augmented_seqs, augmented_descs\n",
    "\n",
    "# 自定义数据集（包含数据增强）\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, descriptions, seq_tokenizer, build_vocab=False, augment=False, config=None):\n",
    "        self.sequences = sequences\n",
    "        self.descriptions = descriptions \n",
    "        self.seq_tokenizer = seq_tokenizer\n",
    "        self.augment = augment\n",
    "        self.config = config\n",
    "        \n",
    "        # 如果需要数据增强\n",
    "        if augment and config:\n",
    "            print(\"Applying data augmentation...\")\n",
    "            self.sequences, self.descriptions = augment_protein_data(sequences, descriptions, config)\n",
    "            print(f\"Augmented dataset size: {len(self.sequences)}\")\n",
    "        \n",
    "        # 构建单词词汇表\n",
    "        if build_vocab:\n",
    "            self.word_counts = self.build_word_counts(self.descriptions)\n",
    "            self.text_tokenizer = WordTokenizer(self.word_counts)\n",
    "        else:\n",
    "            self.text_tokenizer = None\n",
    "    \n",
    "    def build_word_counts(self, descriptions):\n",
    "        \"\"\"从描述文本中统计词频\"\"\"\n",
    "        word_counts = Counter()\n",
    "        for desc in descriptions:\n",
    "            # 简单分词（实际使用应与WordTokenizer相同的分词逻辑）\n",
    "            words = re.findall(r\"\\b[\\w'-]+\\b|[^\\w\\s]\", desc.lower())\n",
    "            word_counts.update(words)\n",
    "        return word_counts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        desc = self.descriptions[idx]\n",
    "        \n",
    "        seq_indices = self.seq_tokenizer.encode(seq)\n",
    "        desc_indices = self.text_tokenizer.encode(desc)\n",
    "        \n",
    "        return {\n",
    "            \"sequence\": torch.tensor(seq_indices, dtype=torch.long),\n",
    "            \"description\": torch.tensor(desc_indices, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 训练函数（带验证集早停）\n",
    "def train(model, train_loader, val_loader, optimizer, device, epochs=50, patience=5):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0  # 早停计数器\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 训练阶段\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            sequences = batch[\"sequence\"].to(device)\n",
    "            descriptions = batch[\"description\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model.p_loss(sequences, descriptions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * sequences.size(0)  # 累计总损失\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # 验证阶段\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch[\"sequence\"].to(device)\n",
    "                descriptions = batch[\"description\"].to(device)\n",
    "                \n",
    "                loss = model.p_loss(sequences, descriptions)\n",
    "                val_loss += loss.item() * sequences.size(0)\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        # 打印日志\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # 早停判断（基于验证损失）\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), \"best_protein_model.pth\")  # 保存最优模型\n",
    "            print(\"Saved best model (val loss improved)\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "                break\n",
    "    \n",
    "    # 加载最优模型\n",
    "    model.load_state_dict(torch.load(\"best_protein_model.pth\"))\n",
    "    return model\n",
    "\n",
    "# 生成函数（根据文本描述生成蛋白质序列）\n",
    "def generate_sequence(model, description, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # 编码文本描述\n",
    "    desc_indices = model.text_tokenizer.encode(description)\n",
    "    desc_tensor = torch.tensor([desc_indices], dtype=torch.long).to(device)  # (1, max_text_len)\n",
    "    \n",
    "    # 采样生成序列索引\n",
    "    with torch.no_grad():\n",
    "        generated_indices = model.p_sample(desc_tensor)  # (1, max_seq_len)\n",
    "    \n",
    "    # 解码为氨基酸序列\n",
    "    return model.seq_tokenizer.decode(generated_indices[0].cpu().numpy())\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 初始化序列分词器\n",
    "    seq_tokenizer = SequenceTokenizer()\n",
    "    print(f\"Sequence vocabulary size: {seq_tokenizer.vocab_size}\")\n",
    "    \n",
    "    # 加载数据\n",
    "    tsv_path = \"uniprot-data.tsv\"\n",
    "    try:\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        print(f\"Loaded TSV with columns: {', '.join(df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading TSV: {e}\")\n",
    "        # 创建示例数据\n",
    "        sample_data = {\n",
    "            \"Sequence\": [\n",
    "                \"MAGLRGLRVGAALAGLAVLGCAVALAVGGGQASFTSQQASALATPGGGQASFTSQQAT\",\n",
    "                \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPR\",\n",
    "                \"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKT\",\n",
    "                \"MGLLCSWSRHCSLHGLGRSAGALRRGPGGPGPLLGLAVLGLSGPGAPGLQALQALRGG\"\n",
    "            ],\n",
    "            \"Protein names\": [\n",
    "                \"Hypothetical protein OS=Homo sapiens\",\n",
    "                \"Transcription factor AP-1 OS=Homo sapiens\",\n",
    "                \"Insulin OS=Homo sapiens\",\n",
    "                \"Collagen alpha-1(I) chain OS=Homo sapiens\"\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        df.to_csv(\"uniprot-data.tsv\", sep='\\t', index=False)\n",
    "        print(\"Created sample dataset: uniprot-data.tsv\")\n",
    "    \n",
    "    # 提取序列和描述\n",
    "    try:\n",
    "        seq_col = \"Sequence\"\n",
    "        desc_col = \"Protein names\"\n",
    "        sequences = df[seq_col].dropna().tolist()\n",
    "        descriptions = df[desc_col].dropna().tolist()\n",
    "        \n",
    "        min_len = min(len(sequences), len(descriptions))\n",
    "        sequences = sequences[:min_len]\n",
    "        descriptions = descriptions[:min_len]\n",
    "        print(f\"Extracted {len(sequences)} raw sequence-description pairs\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing column in TSV: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 过滤短序列\n",
    "    filtered_sequences = []\n",
    "    filtered_descriptions = []\n",
    "    for seq, desc in zip(sequences, descriptions):\n",
    "        if 20 < len(seq) <= config.max_seq_len:\n",
    "            filtered_sequences.append(seq)\n",
    "            filtered_descriptions.append(desc)\n",
    "    print(f\"Filtered to {len(filtered_sequences)} valid pairs (length 20~{config.max_seq_len})\")\n",
    "    \n",
    "    # 拆分训练集和验证集\n",
    "    train_seqs, val_seqs, train_descs, val_descs = train_test_split(\n",
    "        filtered_sequences, \n",
    "        filtered_descriptions, \n",
    "        test_size=config.val_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 创建数据集（在训练集上构建词汇表并应用数据增强）\n",
    "    train_dataset = ProteinDataset(\n",
    "        train_seqs, train_descs, seq_tokenizer, \n",
    "        build_vocab=True, augment=True, config=config\n",
    "    )\n",
    "    text_tokenizer = train_dataset.text_tokenizer\n",
    "    print(f\"\\nText vocabulary size: {text_tokenizer.vocab_size}\")\n",
    "    print(f\"Top 10 words: {text_tokenizer.vocab[4:14]}\")  # 跳过特殊标记\n",
    "    \n",
    "    # 验证集使用相同的词汇表，但不应用数据增强\n",
    "    val_dataset = ProteinDataset(val_seqs, val_descs, seq_tokenizer)\n",
    "    val_dataset.text_tokenizer = text_tokenizer\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    \n",
    "    # 初始化模型和优化器\n",
    "    model = DiffusionModel(config, text_tokenizer, seq_tokenizer).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config.lr,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    print(\"\\nStarting training...\")\n",
    "    model = train(model, train_loader, val_loader, optimizer, device, epochs=50, patience=5)\n",
    "    \n",
    "    # 保存完整模型\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'text_tokenizer': text_tokenizer,\n",
    "        'seq_tokenizer': seq_tokenizer,\n",
    "        'config': config\n",
    "    }, \"protein_diffusion_model_final.pth\")\n",
    "    print(\"\\nModel saved to 'protein_diffusion_model_final.pth'\")\n",
    "    \n",
    "    # 生成示例序列\n",
    "    print(\"\\nGenerating sample sequences...\")\n",
    "    sample_descriptions = [\n",
    "        \"DNA binding protein involved in transcription\",\n",
    "        \"Enzyme with catalytic activity for hydrolysis\",\n",
    "        \"Membrane transport protein for ions\"\n",
    "    ]\n",
    "    \n",
    "    for desc in sample_descriptions:\n",
    "        generated_seq = generate_sequence(model, desc, device)\n",
    "        print(f\"\\nDescription: {desc}\")\n",
    "        print(f\"Generated sequence: {generated_seq[:100]}...\")\n",
    "        print(f\"Sequence length: {len(generated_seq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71375970-9880-4b64-aaad-181ac23dcca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import math\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from collections import Counter\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c848ee-9f6c-4205-8cdb-b58483414836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 超参数配置 - 更新文本相关参数\n",
    "# class Config:\n",
    "#     # 序列相关参数\n",
    "#     seq_vocab_size = 21  # 20种氨基酸 + 1个填充位置\n",
    "#     max_seq_len = 256\n",
    "#     seq_embed_dim = 128\n",
    "    \n",
    "#     # 文本相关参数（单词级）\n",
    "#     text_vocab_size = 5000  # 单词词汇表大小\n",
    "#     max_text_len = 128     # 单词序列长度（更短）\n",
    "#     text_embed_dim = 128\n",
    "    \n",
    "#     # 其他共享参数\n",
    "#     time_embed_dim = 64\n",
    "#     hidden_dim = 256\n",
    "#     num_heads = 8\n",
    "#     num_layers = 6\n",
    "#     dropout = 0.3\n",
    "#     beta_start = 1e-4\n",
    "#     beta_end = 0.02\n",
    "#     T = 1000\n",
    "#     batch_size = 32\n",
    "#     lr = 1e-4\n",
    "#     weight_decay = 1e-5\n",
    "#     val_split = 0.1\n",
    "\n",
    "# config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00da238-46a9-49c6-b056-e690941b62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 文本分词器（单词级）\n",
    "# class WordTokenizer:\n",
    "#     def __init__(self, word_counts=None):\n",
    "#         # 特殊标记\n",
    "#         self.special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "#         self.word2idx = {}\n",
    "#         self.idx2word = {}\n",
    "        \n",
    "#         # 构建词汇表\n",
    "#         self.vocab = self.special_tokens.copy()\n",
    "        \n",
    "#         # 添加高频词\n",
    "#         if word_counts:\n",
    "#             sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "#             for word, _ in sorted_words:\n",
    "#                 if len(self.vocab) < config.text_vocab_size:\n",
    "#                     self.vocab.append(word)\n",
    "#                 else:\n",
    "#                     break\n",
    "        \n",
    "#         # 创建映射\n",
    "#         for i, word in enumerate(self.vocab):\n",
    "#             self.word2idx[word] = i\n",
    "#             self.idx2word[i] = word\n",
    "        \n",
    "#         self.vocab_size = len(self.vocab)\n",
    "    \n",
    "#     def tokenize(self, text):\n",
    "#         \"\"\"将文本分割为单词/标点序列\"\"\"\n",
    "#         # 匹配单词（含连字符）和标点\n",
    "#         return re.findall(r\"\\b[\\w'-]+\\b|[^\\w\\s]\", text.lower())\n",
    "    \n",
    "#     def encode(self, text, max_len=None):\n",
    "#         max_len = max_len or config.max_text_len\n",
    "#         tokens = self.tokenize(text)\n",
    "        \n",
    "#         # 添加起始和结束标记\n",
    "#         tokens = [\"<sos>\"] + tokens[:max_len-2] + [\"<eos>\"]\n",
    "        \n",
    "#         # 转换为索引\n",
    "#         encoded = []\n",
    "#         for token in tokens:\n",
    "#             if token in self.word2idx:\n",
    "#                 encoded.append(self.word2idx[token])\n",
    "#             else:\n",
    "#                 encoded.append(self.word2idx[\"<unk>\"])\n",
    "        \n",
    "#         # 填充或截断\n",
    "#         if len(encoded) < max_len:\n",
    "#             encoded += [self.word2idx[\"<pad>\"]] * (max_len - len(encoded))\n",
    "#         else:\n",
    "#             encoded = encoded[:max_len-1] + [self.word2idx[\"<eos>\"]]  # 确保结束标记\n",
    "        \n",
    "#         return encoded\n",
    "    \n",
    "#     def decode(self, indices):\n",
    "#         \"\"\"将索引序列解码为文本\"\"\"\n",
    "#         words = []\n",
    "#         for idx in indices:\n",
    "#             word = self.idx2word.get(idx, \"<unk>\")\n",
    "#             if word in [\"<pad>\", \"<sos>\", \"<eos>\"]:\n",
    "#                 continue\n",
    "#             words.append(word)\n",
    "#         return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72dbedb-b4f5-4299-ac61-012c3eb683a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 蛋白质序列分词器（保持不变）\n",
    "# class SequenceTokenizer:\n",
    "#     def __init__(self):\n",
    "#         self.aa_list = \"ACDEFGHIKLMNPQRSTVWY\"  # 20种标准氨基酸\n",
    "#         self.pad_token = \"<pad>\"\n",
    "        \n",
    "#         self.aa2idx = {self.pad_token: 0}\n",
    "#         for aa in self.aa_list:\n",
    "#             self.aa2idx[aa] = len(self.aa2idx)\n",
    "        \n",
    "#         self.idx2aa = {v: k for k, v in self.aa2idx.items()}\n",
    "#         self.vocab_size = len(self.aa2idx)\n",
    "    \n",
    "#     def encode(self, sequence, max_len=None):\n",
    "#         max_len = max_len or config.max_seq_len\n",
    "#         encoded = []\n",
    "#         for aa in sequence[:max_len]:\n",
    "#             encoded.append(self.aa2idx.get(aa, self.aa2idx[self.pad_token]))\n",
    "        \n",
    "#         if len(encoded) < max_len:\n",
    "#             encoded += [self.aa2idx[self.pad_token]] * (max_len - len(encoded))\n",
    "#         return encoded\n",
    "    \n",
    "#     def decode(self, indices):\n",
    "#         return ''.join([self.idx2aa.get(idx, 'X') for idx in indices if idx != self.aa2idx[self.pad_token]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f40204-8f3e-4140-b81c-e455253e6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 时间步嵌入（正弦位置编码，用于扩散模型的时间步表示）\n",
    "# class SinusoidalPositionEmbeddings(nn.Module):\n",
    "#     def __init__(self, dim):\n",
    "#         super().__init__()\n",
    "#         self.dim = dim\n",
    "\n",
    "#     def forward(self, t):\n",
    "#         device = t.device\n",
    "#         half_dim = self.dim // 2\n",
    "#         # 计算频率\n",
    "#         embeddings = math.log(10000) / (half_dim - 1)\n",
    "#         embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "#         # 时间步与频率相乘\n",
    "#         embeddings = t[:, None] * embeddings[None, :]\n",
    "#         # 拼接正弦和余弦\n",
    "#         embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "#         return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36927f74-0128-4aad-9b57-d9caf0ecac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 文本编码器（更新为处理单词嵌入）\n",
    "# class TextEncoder(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         # 单词嵌入层\n",
    "#         self.embedding = nn.Embedding(config.text_vocab_size, config.text_embed_dim, padding_idx=0)\n",
    "        \n",
    "#         # Transformer编码器\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(\n",
    "#             d_model=config.text_embed_dim,\n",
    "#             nhead=config.num_heads,\n",
    "#             dim_feedforward=config.hidden_dim,\n",
    "#             dropout=config.dropout,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "#         self.encoder = nn.TransformerEncoder(encoder_layer, config.num_layers)\n",
    "        \n",
    "#     def forward(self, text):\n",
    "#         embedded = self.embedding(text)\n",
    "#         encoded = self.encoder(embedded)\n",
    "#         return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0110f6-f648-47da-8198-462b5fc3ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 蛋白质扩散模型（核心模型：基于文本条件生成蛋白质序列）\n",
    "# class DiffusionModel(nn.Module):\n",
    "#     def __init__(self, config, text_tokenizer, seq_tokenizer):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "#         self.text_tokenizer = text_tokenizer\n",
    "#         self.seq_tokenizer = seq_tokenizer\n",
    "#         self.pad_idx = seq_tokenizer.aa2idx[\"<pad>\"]  # 序列填充索引\n",
    "        \n",
    "#         # 文本条件编码器\n",
    "#         self.text_encoder = TextEncoder(config)\n",
    "        \n",
    "#         # 时间步嵌入\n",
    "#         self.time_embed = nn.Sequential(\n",
    "#             SinusoidalPositionEmbeddings(config.time_embed_dim),\n",
    "#             nn.Linear(config.time_embed_dim, config.time_embed_dim),\n",
    "#             nn.GELU()\n",
    "#         )\n",
    "        \n",
    "#         # 序列嵌入层（使用蛋白质词汇表）\n",
    "#         self.seq_embed = nn.Embedding(\n",
    "#             seq_tokenizer.vocab_size, \n",
    "#             config.seq_embed_dim,\n",
    "#             padding_idx=self.pad_idx\n",
    "#         )\n",
    "        \n",
    "#         # 条件融合：融合文本编码和时间嵌入\n",
    "#         self.condition_fuse = nn.Sequential(\n",
    "#             nn.Linear(config.text_embed_dim + config.time_embed_dim, config.hidden_dim),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(config.dropout),\n",
    "#             nn.Linear(config.hidden_dim, config.seq_embed_dim)  # 映射到序列嵌入维度\n",
    "#         )\n",
    "        \n",
    "#         # Transformer解码器（用于去噪）\n",
    "#         decoder_layer = nn.TransformerDecoderLayer(\n",
    "#             d_model=config.seq_embed_dim,\n",
    "#             nhead=config.num_heads,\n",
    "#             dim_feedforward=config.hidden_dim,\n",
    "#             dropout=config.dropout,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "#         self.decoder = nn.TransformerDecoder(decoder_layer, config.num_layers)\n",
    "        \n",
    "#         # 输出层：将嵌入映射到氨基酸词汇表\n",
    "#         self.output_layer = nn.Linear(config.seq_embed_dim, seq_tokenizer.vocab_size)\n",
    "        \n",
    "#         # 初始化扩散参数（beta/alpha系列）\n",
    "#         self.register_buffer('betas', torch.linspace(config.beta_start, config.beta_end, config.T))\n",
    "#         self.register_buffer('alphas', 1. - self.betas)\n",
    "#         self.register_buffer('alphas_bar', torch.cumprod(self.alphas, dim=0))  # 累积乘积\n",
    "#         self.register_buffer('sqrt_alphas_bar', torch.sqrt(self.alphas_bar))\n",
    "#         self.register_buffer('sqrt_one_minus_alphas_bar', torch.sqrt(1. - self.alphas_bar))\n",
    "#     def forward_emb(self, xt_emb, t, text_condition):\n",
    "#         \"\"\"处理加噪后的嵌入，预测原始嵌入（核心前向逻辑）\"\"\"\n",
    "#         # 1. 时间步嵌入\n",
    "#         t_emb = self.time_embed(t)  # (batch, time_embed_dim)\n",
    "        \n",
    "#         # 2. 文本条件编码\n",
    "#         text_encoded = self.text_encoder(text_condition)  # (batch, text_len, text_embed_dim)\n",
    "        \n",
    "#         # 3. 融合文本和时间条件\n",
    "#         # 文本编码取平均作为全局特征\n",
    "#         text_avg = text_encoded.mean(dim=1)  # (batch, text_embed_dim)\n",
    "#         # 拼接文本和时间嵌入\n",
    "#         cond = torch.cat([text_avg, t_emb], dim=-1)  # (batch, text_embed_dim + time_embed_dim)\n",
    "#         cond = self.condition_fuse(cond)  # (batch, seq_embed_dim)\n",
    "#         # 扩展到序列长度（与xt_emb对齐）\n",
    "#         cond = cond.unsqueeze(1).repeat(1, xt_emb.size(1), 1)  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "#         # 4. 融合加噪嵌入与条件\n",
    "#         x_cond = xt_emb + cond  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "#         # 5. Transformer解码（用文本编码作为memory增强条件）\n",
    "#         output_emb = self.decoder(tgt=x_cond, memory=text_encoded)  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "#         return output_emb\n",
    "\n",
    "\n",
    "#     def p_loss(self, x0, text_condition):\n",
    "#         \"\"\"计算扩散损失（基于嵌入空间的MSE）\"\"\"\n",
    "#         batch_size = x0.size(0)\n",
    "#         device = x0.device\n",
    "        \n",
    "#         # 1. 随机采样时间步\n",
    "#         t = torch.randint(0, self.config.T, (batch_size,), device=device)\n",
    "        \n",
    "#         # 2. 原始序列嵌入\n",
    "#         x0_emb = self.seq_embed(x0)  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "#         # 3. 前向扩散：在嵌入空间添加噪声\n",
    "#         noise = torch.randn_like(x0_emb)  # 高斯噪声\n",
    "#         # 提取当前时间步的系数\n",
    "#         sqrt_alpha_bar = self.sqrt_alphas_bar[t][:, None, None]  # (batch, 1, 1)\n",
    "#         sqrt_one_minus_alpha_bar = self.sqrt_one_minus_alphas_bar[t][:, None, None]  # (batch, 1, 1)\n",
    "#         # 计算加噪嵌入xt_emb\n",
    "#         xt_emb = sqrt_alpha_bar * x0_emb + sqrt_one_minus_alpha_bar * noise  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "#         # 4. 模型预测原始嵌入\n",
    "#         pred_x0_emb = self.forward_emb(xt_emb, t, text_condition)  # (batch, seq_len, seq_embed_dim)\n",
    "        \n",
    "#         # 5. 计算MSE损失（忽略填充位置）\n",
    "#         mask = (x0 != self.pad_idx).float().unsqueeze(-1)  # (batch, seq_len, 1)：填充位置为0\n",
    "#         loss = F.mse_loss(pred_x0_emb * mask, x0_emb * mask, reduction='mean')\n",
    "        \n",
    "#         return loss\n",
    "\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def p_sample(self, text_condition):\n",
    "#         \"\"\"反向扩散采样：从噪声嵌入生成蛋白质序列\"\"\"\n",
    "#         device = next(self.parameters()).device\n",
    "#         batch_size = text_condition.size(0)\n",
    "#         seq_len = self.config.max_seq_len\n",
    "        \n",
    "#         # 1. 初始噪声嵌入（从标准正态分布采样）\n",
    "#         xt_emb = torch.randn((batch_size, seq_len, self.config.seq_embed_dim), device=device)\n",
    "        \n",
    "#         # 2. 逐步去噪\n",
    "#         for t_step in reversed(range(self.config.T)):\n",
    "#             t = torch.full((batch_size,), t_step, device=device, dtype=torch.long)  # 当前时间步\n",
    "            \n",
    "#             # 预测原始嵌入\n",
    "#             pred_x0_emb = self.forward_emb(xt_emb, t, text_condition)  # (batch, seq_len, seq_embed_dim)\n",
    "            \n",
    "#             # 计算反向扩散系数\n",
    "#             alpha = self.alphas[t_step]\n",
    "#             alpha_bar = self.alphas_bar[t_step]\n",
    "#             beta = self.betas[t_step]\n",
    "            \n",
    "#             # 采样噪声（最后一步用0噪声）\n",
    "#             if t_step > 0:\n",
    "#                 noise = torch.randn_like(xt_emb)\n",
    "#             else:\n",
    "#                 noise = torch.zeros_like(xt_emb)\n",
    "            \n",
    "#             # 反向更新公式：xt-1 = (1/sqrt(alpha)) * (xt - (1-alpha)/sqrt(1-alpha_bar) * pred_x0) + sqrt(beta) * noise\n",
    "#             xt_emb = (1 / torch.sqrt(alpha)) * (\n",
    "#                 xt_emb - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * pred_x0_emb\n",
    "#             ) + torch.sqrt(beta) * noise\n",
    "        \n",
    "#         # 3. 将最终嵌入映射到氨基酸索引\n",
    "#         logits = self.output_layer(xt_emb)  # (batch, seq_len, vocab_size)\n",
    "#         return torch.argmax(logits, dim=-1)  # 取概率最大的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304158c1-6f9d-4671-90be-5353bd3ca2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 自定义数据集（添加词汇表构建）\n",
    "# class ProteinDataset(Dataset):\n",
    "#     def __init__(self, sequences, descriptions, seq_tokenizer, build_vocab=False):\n",
    "#         self.sequences = sequences\n",
    "#         self.descriptions = descriptions \n",
    "#         self.seq_tokenizer = seq_tokenizer\n",
    "        \n",
    "#         # 构建单词词汇表\n",
    "#         if build_vocab:\n",
    "#             self.word_counts = self.build_word_counts(descriptions)\n",
    "#             self.text_tokenizer = WordTokenizer(self.word_counts)\n",
    "#         else:\n",
    "#             self.text_tokenizer = None\n",
    "    \n",
    "#     def build_word_counts(self, descriptions):\n",
    "#         \"\"\"从描述文本中统计词频\"\"\"\n",
    "#         word_counts = Counter()\n",
    "#         for desc in descriptions:\n",
    "#             # 简单分词（实际使用应与WordTokenizer相同的分词逻辑）\n",
    "#             words = re.findall(r\"\\b[\\w'-]+\\b|[^\\w\\s]\", desc.lower())\n",
    "#             word_counts.update(words)\n",
    "#         return word_counts\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         seq = self.sequences[idx]\n",
    "#         desc = self.descriptions[idx]\n",
    "        \n",
    "#         seq_indices = self.seq_tokenizer.encode(seq)\n",
    "#         desc_indices = self.text_tokenizer.encode(desc)\n",
    "        \n",
    "#         return {\n",
    "#             \"sequence\": torch.tensor(seq_indices, dtype=torch.long),\n",
    "#             \"description\": torch.tensor(desc_indices, dtype=torch.long)\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc0810-25a8-4d18-beee-a1fef023bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 5. 训练函数\n",
    "# # def train(model, dataloader, optimizer, device, epochs=10):\n",
    "# #     model.train()\n",
    "# #     for epoch in range(epochs):\n",
    "# #         total_loss = 0\n",
    "# #         for batch in dataloader:\n",
    "# #             sequences = batch[\"sequence\"].to(device)\n",
    "# #             descriptions = batch[\"description\"].to(device)\n",
    "            \n",
    "# #             optimizer.zero_grad()\n",
    "# #             loss = model.p_loss(sequences, descriptions)\n",
    "# #             loss.backward()\n",
    "# #             optimizer.step()\n",
    "            \n",
    "# #             total_loss += loss.item()\n",
    "        \n",
    "# #         print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "# # 训练函数（带验证集早停）\n",
    "# def train(model, train_loader, val_loader, optimizer, device, epochs=50, patience=5):\n",
    "#     model.train()\n",
    "#     best_val_loss = float('inf')\n",
    "#     counter = 0  # 早停计数器\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         # 训练阶段\n",
    "#         train_loss = 0.0\n",
    "#         model.train()\n",
    "#         for batch in train_loader:\n",
    "#             sequences = batch[\"sequence\"].to(device)\n",
    "#             descriptions = batch[\"description\"].to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss = model.p_loss(sequences, descriptions)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_loss += loss.item() * sequences.size(0)  # 累计总损失\n",
    "#         avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "#         # 验证阶段\n",
    "#         val_loss = 0.0\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 sequences = batch[\"sequence\"].to(device)\n",
    "#                 descriptions = batch[\"description\"].to(device)\n",
    "                \n",
    "#                 loss = model.p_loss(sequences, descriptions)\n",
    "#                 val_loss += loss.item() * sequences.size(0)\n",
    "#         avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "#         # 打印日志\n",
    "#         print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "#         print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "#         # 早停判断（基于验证损失）\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             counter = 0\n",
    "#             torch.save(model.state_dict(), \"best_protein_model.pth\")  # 保存最优模型\n",
    "#             print(\"Saved best model (val loss improved)\")\n",
    "#         else:\n",
    "#             counter += 1\n",
    "#             if counter >= patience:\n",
    "#                 print(f\"Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "#                 break\n",
    "    \n",
    "#     # 加载最优模型\n",
    "#     model.load_state_dict(torch.load(\"best_protein_model.pth\"))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3850be-3754-4188-8340-30df6f4099ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成函数（根据文本描述生成蛋白质序列）\n",
    "# def generate_sequence(model, description, device):\n",
    "#     model.eval()\n",
    "    \n",
    "#     # 编码文本描述\n",
    "#     desc_indices = model.text_tokenizer.encode(description)\n",
    "#     desc_tensor = torch.tensor([desc_indices], dtype=torch.long).to(device)  # (1, max_text_len)\n",
    "    \n",
    "#     # 采样生成序列索引\n",
    "#     with torch.no_grad():\n",
    "#         generated_indices = model.p_sample(desc_tensor)  # (1, max_seq_len)\n",
    "    \n",
    "#     # 解码为氨基酸序列\n",
    "#     return model.seq_tokenizer.decode(generated_indices[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492bf56-e38d-48ab-b73e-758330d8629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 主程序（更新数据加载逻辑）\n",
    "# if __name__ == \"__main__\":\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     print(f\"Using device: {device}\")\n",
    "    \n",
    "#     # 初始化序列分词器\n",
    "#     seq_tokenizer = SequenceTokenizer()\n",
    "#     print(f\"Sequence vocabulary size: {seq_tokenizer.vocab_size}\")\n",
    "    \n",
    "#     # 加载数据\n",
    "#     tsv_path = \"uniprot-data.tsv\"\n",
    "#     try:\n",
    "#         df = pd.read_csv(tsv_path, sep='\\t')\n",
    "#         print(f\"Loaded TSV with columns: {', '.join(df.columns)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading TSV: {e}\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     # 提取序列和描述\n",
    "#     try:\n",
    "#         seq_col = \"Sequence\"\n",
    "#         desc_col = \"Protein names\"\n",
    "#         sequences = df[seq_col].dropna().tolist()\n",
    "#         descriptions = df[desc_col].dropna().tolist()\n",
    "        \n",
    "#         min_len = min(len(sequences), len(descriptions))\n",
    "#         sequences = sequences[:min_len]\n",
    "#         descriptions = descriptions[:min_len]\n",
    "#         print(f\"Extracted {len(sequences)} raw sequence-description pairs\")\n",
    "#     except KeyError as e:\n",
    "#         print(f\"Missing column in TSV: {e}\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     # 过滤短序列\n",
    "#     filtered_sequences = []\n",
    "#     filtered_descriptions = []\n",
    "#     for seq, desc in zip(sequences, descriptions):\n",
    "#         if 20 < len(seq) <= config.max_seq_len:\n",
    "#             filtered_sequences.append(seq)\n",
    "#             filtered_descriptions.append(desc)\n",
    "#     print(f\"Filtered to {len(filtered_sequences)} valid pairs (length 20~{config.max_seq_len})\")\n",
    "    \n",
    "#     # 拆分训练集和验证集\n",
    "#     train_seqs, val_seqs, train_descs, val_descs = train_test_split(\n",
    "#         filtered_sequences, \n",
    "#         filtered_descriptions, \n",
    "#         test_size=config.val_split,\n",
    "#         random_state=42\n",
    "#     )\n",
    "    \n",
    "#     # 创建数据集（在训练集上构建词汇表）\n",
    "#     train_dataset = ProteinDataset(train_seqs, train_descs, seq_tokenizer, build_vocab=True)\n",
    "#     text_tokenizer = train_dataset.text_tokenizer\n",
    "#     print(f\"\\nText vocabulary size: {text_tokenizer.vocab_size}\")\n",
    "#     print(f\"Top 10 words: {text_tokenizer.vocab[4:14]}\")  # 跳过特殊标记\n",
    "    \n",
    "#     # 验证集使用相同的词汇表\n",
    "#     val_dataset = ProteinDataset(val_seqs, val_descs, seq_tokenizer)\n",
    "#     val_dataset.text_tokenizer = text_tokenizer\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    \n",
    "#     # 初始化模型和优化器\n",
    "#     model = DiffusionModel(config, text_tokenizer, seq_tokenizer).to(device)\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         model.parameters(), \n",
    "#         lr=config.lr,\n",
    "#         weight_decay=config.weight_decay\n",
    "#     )\n",
    "    \n",
    "#     # 训练模型\n",
    "#     print(\"\\nStarting training...\")\n",
    "#     model = train(model, train_loader, val_loader, optimizer, device, epochs=50, patience=5)\n",
    "    \n",
    "#     # 保存完整模型\n",
    "#     torch.save({\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'text_tokenizer': text_tokenizer,\n",
    "#         'seq_tokenizer': seq_tokenizer,\n",
    "#         'config': config\n",
    "#     }, \"protein_diffusion_model_final.pth\")\n",
    "#     print(\"\\nModel saved to 'protein_diffusion_model_final.pth'\")\n",
    "    \n",
    "#     # 生成示例序列\n",
    "#     print(\"\\nGenerating sample sequences...\")\n",
    "#     sample_descriptions = [\n",
    "#         \"DNA binding protein involved in transcription\",\n",
    "#         \"Enzyme with catalytic activity for hydrolysis\",\n",
    "#         \"Membrane transport protein for ions\"\n",
    "#     ]\n",
    "    \n",
    "#     for desc in sample_descriptions:\n",
    "#         generated_seq = generate_sequence(model, desc, device)\n",
    "#         print(f\"\\nDescription: {desc}\")\n",
    "#         print(f\"Generated sequence: {generated_seq[:100]}...\")\n",
    "#         print(f\"Sequence length: {len(generated_seq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a5558-de8c-4357-bc34-3e4e475ab3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:protein_diffusion_env]",
   "language": "python",
   "name": "conda-env-protein_diffusion_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
